{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import dill\n",
    "import tqdm\n",
    "import gc\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text, re.I)\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = '((https?:\\/\\/)(\\s)*(www\\.)?|(www\\.))(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*'\n",
    "    text = re.sub(url_pattern, ' ', text, re.I)\n",
    "    return text\n",
    "\n",
    "def remove_checklists(text):\n",
    "    checklist_pattern = r'\\[[xX\\.\\s]\\]'\n",
    "    text = re.sub(checklist_pattern, ' ', text, re.I|re.DOTALL)\n",
    "    return text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def pre_process_document(document):\n",
    "    \n",
    "    # strip HTML\n",
    "    document = strip_html_tags(document)\n",
    "    \n",
    "    # remove URLS\n",
    "    document = remove_urls(document)\n",
    "    \n",
    "    # remove checklists\n",
    "    document = remove_checklists(document)\n",
    "    \n",
    "    # expand contractions    \n",
    "    document = expand_contractions(document)\n",
    "    \n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    \n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    \n",
    "    # remove accented characters\n",
    "    document = remove_accented_chars(document)\n",
    "    \n",
    "    \n",
    "               \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    document = remove_special_characters(document, remove_digits=False)  \n",
    "    \n",
    "    # remove only numbers\n",
    "    document = re.sub(r'\\b\\d+\\b', ' ', document)\n",
    "        \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsarkar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "EMBED_SIZE = 300 # how big is each word vector\n",
    "CVE_MAX_FEATURES = 557002 # how many unique words to use (i.e num rows in embedding vector)\n",
    "SEC_MAX_FEATURES = 800000\n",
    "MAX_LEN = 1000 # max number of words in a doc to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preloading\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "CVE_WORD2IDX_MAP_FILE = '../../../tokenizer_vocab/cve_tokenizer_word2idx.pkl'\n",
    "SEC_WORD2IDX_MAP_FILE = '../../../tokenizer_vocab/sec_tokenizer_word2idx.pkl'\n",
    "\n",
    "print('preloading')\n",
    "cve_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=CVE_MAX_FEATURES)\n",
    "sec_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=SEC_MAX_FEATURES)\n",
    "\n",
    "with open(CVE_WORD2IDX_MAP_FILE, 'rb') as f:\n",
    "    word2idx = dill.load(f)\n",
    "cve_tokenizer.word_index = word2idx\n",
    "\n",
    "\n",
    "with open(SEC_WORD2IDX_MAP_FILE, 'rb') as f:\n",
    "    word2idx = dill.load(f)\n",
    "sec_tokenizer.word_index = word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = keras.regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = keras.constraints.get(W_constraint)\n",
    "        self.b_constraint = keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        # old code doesn't work\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), \n",
    "                              K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'step_dim': self.step_dim}\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cve_cpumodel(embedding_size, max_len, max_features, gru_units=32):\n",
    "    \n",
    "    inp = keras.layers.Input(shape=(max_len,))\n",
    "    x = keras.layers.Embedding(max_features, embedding_size,trainable=True)(inp)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units*2, return_sequences=True, \n",
    "                                                    reset_after=True, recurrent_activation='sigmoid'))(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units, return_sequences=True, reset_after=True, \n",
    "                                                    recurrent_activation='sigmoid'))(x)\n",
    "    x = AttentionLayer(max_len)(x)\n",
    "    x = keras.layers.Dense(gru_units, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.1)(x)\n",
    "    x = keras.layers.Dense(gru_units//2, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.1)(x)\n",
    "    outp = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    # initialize the model\n",
    "    model = keras.models.Model(inputs=inp, outputs=outp)       \n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_sec_cpumodel(embedding_size, max_len, max_features, gru_units=32):\n",
    "    \n",
    "    inp = keras.layers.Input(shape=(max_len,))\n",
    "    x = keras.layers.Embedding(max_features, embedding_size,trainable=True)(inp)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units, return_sequences=True, reset_after=True, \n",
    "                                                    recurrent_activation='sigmoid'))(x)\n",
    "    x = AttentionLayer(max_len)(x)\n",
    "    x = keras.layers.Dense(gru_units, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "    outp = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    # initialize the model\n",
    "    model = keras.models.Model(inputs=inp, outputs=outp)       \n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu:0'):\n",
    "    \n",
    "    gru_sec_model = build_sec_cpumodel(embedding_size=EMBED_SIZE, \n",
    "                                       max_len=MAX_LEN, max_features=SEC_MAX_FEATURES, \n",
    "                                       gru_units=32)\n",
    "    gru_sec_model.load_weights('../../../models/model1_sec_nonsec_demo_weights2.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    gru_cve_model = build_cve_cpumodel(embedding_size=EMBED_SIZE, \n",
    "                                       max_len=MAX_LEN, max_features=CVE_MAX_FEATURES, \n",
    "                                       gru_units=32)\n",
    "    gru_cve_model.load_weights('../../../models/model2_cve_noncve_demo_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [\"Bug 1622372 - Require CSRF token on all proxied requests * Require a CSRF token on all proxied requests. This prevents loading\\r\\n  content hosted from a pod under the console domain by clicking on a\\r\\n  link that uses the console proxy. Previously, it was not required for\\r\\n  GET requests.\\r\\n* Do not forward the X-CSRFToken header through the proxy.\\r\\n* Set `Content-Security-Policy: default-src 'none'` in the proxied\\r\\n  response to prevent scripts from running in proxied content.\\r\\n\\r\\nIn order to support the CSRF token for WebSockets, this adds an\\r\\n`x-csrf-token` query parameter when headers can't be set. It also updates\\r\\nthe console to check the `Origin` header when present since `Referer` is\\r\\nnot set for WebSockets.\\r\\n\\r\\nFixes https://bugzilla.redhat.com/show_bug.cgi?id=1622372\\r\\n\\r\\n/assign @liggitt \"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bug require csrf token on all proxied requests require a csrf token on all proxied requests this prevents loading content hosted from a pod under the console domain by clicking on a link that uses the console proxy previously it was not required for get requests do not forward the x csrftoken header through the proxy set content security policy default src none in the proxied response to prevent scripts from running in proxied content in order to support the csrf token for websockets this adds an x csrf token query parameter when headers cannot be set it also updates the console to check the origin header when present since referer is not set for websockets fixes cgi id assign liggitt'],\n",
       "      dtype='<U693')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd = pre_process_corpus(d)\n",
    "nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_nd = sec_tokenizer.texts_to_sequences(nd)\n",
    "sec_nd = keras.preprocessing.sequence.pad_sequences(sec_nd, maxlen=MAX_LEN)\n",
    "sec_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = gru_sec_model.predict(sec_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1], array([[0.99915385]], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_yr = pred_y.ravel()\n",
    "pred_yl = [1 if prob > 1e-3 else 0 for prob in pred_yr]\n",
    "pred_yl, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999999]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cve_nd = cve_tokenizer.texts_to_sequences(nd)\n",
    "cve_nd = keras.preprocessing.sequence.pad_sequences(cve_nd, maxlen=MAX_LEN)\n",
    "cve_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = gru_cve_model.predict(cve_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1], array([[0.99915385]], dtype=float32))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_yr = pred_y.ravel()\n",
    "pred_yl = [1 if prob > 1e-3 else 0 for prob in pred_yr]\n",
    "pred_yl, pred_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
