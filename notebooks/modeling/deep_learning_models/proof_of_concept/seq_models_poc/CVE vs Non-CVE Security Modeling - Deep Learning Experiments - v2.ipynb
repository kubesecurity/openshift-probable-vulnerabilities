{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOF reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(747518, 747518)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/gh_descs_norm.pkl', 'rb') as f:\n",
    "    data = []\n",
    "    while True:\n",
    "        try:\n",
    "            data.extend(dill.load(f))\n",
    "        except:\n",
    "            print('EOF reached')\n",
    "            break\n",
    "            \n",
    "with open('./data/gh_labels.pkl', 'rb') as f:\n",
    "    labels = dill.load(f)\n",
    "    \n",
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107600, 107600)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data = []\n",
    "positive_labels = []\n",
    "for doc, label in zip(data, labels):\n",
    "    if label != 0:\n",
    "        positive_data.append(doc)\n",
    "        positive_labels.append(label)\n",
    "        \n",
    "len(positive_data), len(positive_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pip should not execute arbitrary code from the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make load safe_load - [ ] Make `yaml.load` def...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CSRF vulnerability CVE-2013-7259 Hi, I'm looki...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Add salt to Array#hash For #2437, we partially...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cross Site Scripting Vulnerability Versions up...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  label\n",
       "0  pip should not execute arbitrary code from the...      2\n",
       "1  Make load safe_load - [ ] Make `yaml.load` def...      2\n",
       "2  CSRF vulnerability CVE-2013-7259 Hi, I'm looki...      2\n",
       "3  Add salt to Array#hash For #2437, we partially...      2\n",
       "4  Cross Site Scripting Vulnerability Versions up...      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/GH_processed_labeled_issues_prs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "del labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96840, 10760)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(positive_data, positive_labels, \n",
    "                                                    test_size=0.1, random_state=SEED)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "EMBED_SIZE = 300 # how big is each word vector\n",
    "MAX_FEATURES = 500000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_LEN = 1000 # max number of words in a doc to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "CVE_WORD2IDX_MAP_FILE = 'cve_tokenizer_word2idx.pkl'\n",
    "\n",
    "if not os.path.isfile(CVE_WORD2IDX_MAP_FILE):\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=MAX_FEATURES)\n",
    "    tokenizer.fit_on_texts(list(X_train))\n",
    "    tokenizer.word_index['<PAD>'] = 0\n",
    "    with open(CVE_WORD2IDX_MAP_FILE, 'wb') as f:\n",
    "        dill.dump(tokenizer.word_index, f)\n",
    "else:\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "    with open(CVE_WORD2IDX_MAP_FILE, 'rb') as f:\n",
    "        word2idx = dill.load(f)\n",
    "    tokenizer.word_index = word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_FEATURES = len(tokenizer.word_index)\n",
    "MAX_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the sentences\n",
    "train_X = tokenizer.texts_to_sequences(X_train)\n",
    "test_X = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pad the sentences \n",
    "train_X = keras.preprocessing.sequence.pad_sequences(train_X, maxlen=MAX_LEN)\n",
    "test_X = keras.preprocessing.sequence.pad_sequences(test_X, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96840, 1000), (10760, 1000))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array([1 if item==2 else 0 for item in y_train])\n",
    "test_y = np.array([1 if item==2 else 0 for item in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61008, 74836,  7646, ..., 76820,   860, 15795])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = np.random.permutation(len(train_X))\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96840, 1000), (96840,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_X[train_idx]\n",
    "train_y = train_y[train_idx]\n",
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):    \n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*row.split(\" \")) \n",
    "                                for row in open(embedding_file_path, encoding=\"utf8\", errors='ignore') \n",
    "                                    if len(row)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_to_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx >= max_features: \n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557002, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FASTTEXT_INIT_EMBEDDINGS_FILE = 'fasttext_init_embeddings.pkl'\n",
    "\n",
    "if not os.path.isfile(FASTTEXT_INIT_EMBEDDINGS_FILE):\n",
    "    FASTTEXT_EMBEDDINGS_PATH = './embeddings/fasttext/crawl-300d-2M.vec'\n",
    "    ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, max_features=MAX_FEATURES, \n",
    "                                               embedding_size=EMBED_SIZE, \n",
    "                                               embedding_file_path=FASTTEXT_EMBEDDINGS_PATH)\n",
    "    with open(FASTTEXT_INIT_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        dill.dump(ft_embeddings, f)\n",
    "else:\n",
    "    with open(FASTTEXT_INIT_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        ft_embeddings = dill.load(f)\n",
    "        \n",
    "ft_embeddings.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557002, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAGRAM_INIT_EMBEDDINGS_FILE = 'paragram_init_embeddings.pkl'\n",
    "\n",
    "if not os.path.isfile(PARAGRAM_INIT_EMBEDDINGS_FILE):\n",
    "    PARAGRAM_EMBEDDINGS_PATH = './embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    pg_embeddings = load_pretrained_embeddings(word_to_index=word2idx, max_features=MAX_FEATURES, \n",
    "                                               embedding_size=EMBED_SIZE, \n",
    "                                               embedding_file_path=PARAGRAM_EMBEDDINGS_PATH)\n",
    "    with open(PARAGRAM_INIT_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        dill.dump(pg_embeddings, f)\n",
    "else:\n",
    "    with open(PARAGRAM_INIT_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        pg_embeddings = dill.load(f)\n",
    "        \n",
    "pg_embeddings.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557002, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pretrained_embeddings = np.mean([ft_embeddings, pg_embeddings], axis = 0)\n",
    "avg_pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = keras.regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = keras.constraints.get(W_constraint)\n",
    "        self.b_constraint = keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        # old code doesn't work\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), \n",
    "                              K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'step_dim': self.step_dim}\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "def build_gru_model(embedding_matrix, embedding_size, max_len, max_features, gru_units=32):\n",
    "    \n",
    "    inp = keras.layers.Input(shape=(max_len,))\n",
    "    x = keras.layers.Embedding(max_features, embedding_size, \n",
    "                                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = keras.layers.Bidirectional(keras.layers.CuDNNGRU(gru_units*2, return_sequences=True))(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.CuDNNGRU(gru_units, return_sequences=True))(x)\n",
    "    x = AttentionLayer(max_len)(x)\n",
    "    x = keras.layers.Dense(gru_units, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.1)(x)\n",
    "    x = keras.layers.Dense(gru_units//2, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.1)(x)\n",
    "    outp = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    # initialize the model\n",
    "    model = keras.models.Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    # make the model parallel\n",
    "    #model = multi_gpu_model(model, gpus=2)\n",
    "       \n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         167100600 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 128)         140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1000, 64)          31104     \n",
      "_________________________________________________________________\n",
      "attention_layer_1 (Attention (None, 64)                1064      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 167,275,937\n",
      "Trainable params: 167,275,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = build_gru_model(embedding_matrix=avg_pretrained_embeddings, embedding_size=EMBED_SIZE, \n",
    "                            max_len=MAX_LEN, max_features=MAX_FEATURES, gru_units=32)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5031433470151192, 1: 160.06611570247935}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_y),\n",
    "                                                 train_y)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights[1] *= 2\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 167100600 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87156 samples, validate on 9684 samples\n",
      "Epoch 1/15\n",
      "87156/87156 [==============================] - 217s 2ms/step - loss: 0.8379 - acc: 0.3211 - val_loss: 0.7496 - val_acc: 0.4650\n",
      "Epoch 2/15\n",
      "87156/87156 [==============================] - 209s 2ms/step - loss: 0.4662 - acc: 0.7773 - val_loss: 0.7223 - val_acc: 0.8772\n",
      "Epoch 3/15\n",
      "87156/87156 [==============================] - 209s 2ms/step - loss: 0.2084 - acc: 0.9187 - val_loss: 0.8512 - val_acc: 0.8153\n",
      "Epoch 4/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.1562 - acc: 0.9414 - val_loss: 1.3154 - val_acc: 0.9371\n",
      "Epoch 5/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0902 - acc: 0.9667 - val_loss: 1.9824 - val_acc: 0.9585\n",
      "Epoch 6/15\n",
      "87156/87156 [==============================] - 209s 2ms/step - loss: 0.0728 - acc: 0.9728 - val_loss: 2.7702 - val_acc: 0.9683\n",
      "Epoch 7/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0493 - acc: 0.9813 - val_loss: 3.0324 - val_acc: 0.9679\n",
      "Epoch 8/15\n",
      "87156/87156 [==============================] - 209s 2ms/step - loss: 0.0431 - acc: 0.9849 - val_loss: 3.5358 - val_acc: 0.9736\n",
      "Epoch 9/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0506 - acc: 0.9859 - val_loss: 2.5224 - val_acc: 0.9709\n",
      "Epoch 10/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0362 - acc: 0.9838 - val_loss: 5.2154 - val_acc: 0.9769\n",
      "Epoch 11/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0589 - acc: 0.9795 - val_loss: 2.7892 - val_acc: 0.9706\n",
      "Epoch 12/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0345 - acc: 0.9901 - val_loss: 2.9917 - val_acc: 0.9718\n",
      "Epoch 13/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0182 - acc: 0.9930 - val_loss: 5.0323 - val_acc: 0.9783\n",
      "Epoch 14/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0121 - acc: 0.9959 - val_loss: 5.8712 - val_acc: 0.9850\n",
      "Epoch 15/15\n",
      "87156/87156 [==============================] - 210s 2ms/step - loss: 0.0149 - acc: 0.9954 - val_loss: 6.1677 - val_acc: 0.9843\n"
     ]
    }
   ],
   "source": [
    "history = gru_model.fit(train_X, train_y, batch_size=512, epochs=15, \n",
    "                        class_weight=class_weights, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10760/10760 [==============================] - 8s 771us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.3561016e-07],\n",
       "       [9.4871466e-06],\n",
       "       [1.7131248e-05],\n",
       "       ...,\n",
       "       [1.4889696e-04],\n",
       "       [8.3001629e-07],\n",
       "       [2.6304090e-06]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = gru_model.predict([test_X], batch_size=512, verbose=1)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yr = pred_y.ravel()\n",
    "pred_yl = [1 if prob > 0.001 else 0 for prob in pred_yr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10317,   392],\n",
       "       [   27,    24]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=test_y, y_pred=pred_yl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98     10709\n",
      "          1       0.06      0.47      0.10        51\n",
      "\n",
      "avg / total       0.99      0.96      0.98     10760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=test_y, y_pred=pred_yl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred_train_y = gru_model.predict([train_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(train_y, \n",
    "                                                                             (pred_train_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_test_y = gru_model.predict([test_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.0001, 0.501, 0.0001):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(test_y, \n",
    "                                                                             (pred_test_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.save('./models/model2_cve_noncve_demo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.save_weights('./models/model2_cve_noncve_demo_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_cpumodel(embedding_size, max_len, max_features, gru_units=32):\n",
    "    \n",
    "    inp = keras.layers.Input(shape=(max_len,))\n",
    "    x = keras.layers.Embedding(max_features, embedding_size, trainable=True)(inp)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units*2, return_sequences=True, \n",
    "                                                    reset_after=True, recurrent_activation='sigmoid'))(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units, return_sequences=True, reset_after=True, \n",
    "                                                    recurrent_activation='sigmoid'))(x)\n",
    "    x = AttentionLayer(max_len)(x)\n",
    "    x = keras.layers.Dense(gru_units, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = keras.layers.Dense(gru_units // 2, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "    outp = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    # initialize the model\n",
    "    model = keras.models.Model(inputs=inp, outputs=outp)       \n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 300)         167100600 \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 1000, 128)         140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 1000, 64)          31104     \n",
      "_________________________________________________________________\n",
      "attention_layer_4 (Attention (None, 64)                1064      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 167,275,937\n",
      "Trainable params: 167,275,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_cpu_model = build_gru_cpumodel(embedding_size=EMBED_SIZE, \n",
    "                                   max_len=MAX_LEN, max_features=MAX_FEATURES, gru_units=32)\n",
    "gru_cpu_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_cpu_model.load_weights('./models/model2_cve_noncve_demo_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10760/10760 [==============================] - 27s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.3560991e-07],\n",
       "       [9.4872184e-06],\n",
       "       [1.7131671e-05],\n",
       "       ...,\n",
       "       [1.4889981e-04],\n",
       "       [8.3001549e-07],\n",
       "       [2.6304187e-06]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = gru_cpu_model.predict([test_X], batch_size=1024, verbose=1)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10317,   392],\n",
       "       [   27,    24]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_yr = pred_y.ravel()\n",
    "pred_yl = [1 if prob > 1e-3 else 0 for prob in pred_yr]\n",
    "confusion_matrix(y_true=test_y, y_pred=pred_yl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
